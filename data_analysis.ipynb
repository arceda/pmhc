{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisis de los tamaños de los petido y MHC, con el objetivo de definir un tamaño maximo y hacer padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "max peptide length 37\n",
      "min peptide length 9\n",
      "mean peptide length 15.313720551257678\n",
      "max MHC length 34\n",
      "min MHC length 34\n",
      "\n",
      "TEST\n",
      "max peptide length 37\n",
      "min peptide length 9\n",
      "max MHC length 34\n",
      "min MHC length 34\n",
      "\n",
      "VAL\n",
      "max peptide length 37\n",
      "min peptide length 9\n",
      "max MHC length 34\n",
      "min MHC length 34\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../dataset/netMHCIIpan3.2/train.csv')\n",
    "#print(data)\n",
    "\n",
    "print(\"TRAIN\")\n",
    "print(f\"max peptide length {data.peptide.str.len().max()}\")\n",
    "print(f\"min peptide length {data.peptide.str.len().min()}\")\n",
    "print(f\"mean peptide length {data.peptide.str.len().mean()}\")\n",
    "print(f\"max MHC length {data.mhc.str.len().max()}\")\n",
    "print(f\"min MHC length {data.mhc.str.len().min()}\")\n",
    "\n",
    "\n",
    "data = pd.read_csv('../dataset/netMHCIIpan3.2/test.csv')\n",
    "#print(data)\n",
    "\n",
    "print(\"\\nTEST\")\n",
    "print(f\"max peptide length {data.peptide.str.len().max()}\")\n",
    "print(f\"min peptide length {data.peptide.str.len().min()}\")\n",
    "print(f\"max MHC length {data.mhc.str.len().max()}\")\n",
    "print(f\"min MHC length {data.mhc.str.len().min()}\")\n",
    "\n",
    "\n",
    "data = pd.read_csv('../dataset/netMHCIIpan3.2/eval.csv')\n",
    "#print(data)\n",
    "\n",
    "print(\"\\nVAL\")\n",
    "print(f\"max peptide length {data.peptide.str.len().max()}\")\n",
    "print(f\"min peptide length {data.peptide.str.len().min()}\")\n",
    "print(f\"max MHC length {data.mhc.str.len().max()}\")\n",
    "print(f\"min MHC length {data.mhc.str.len().min()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAPE with Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type esm to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"base_model\": \"transformer\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"input_size\": 768,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_size\": 768,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 30\n",
      "}\n",
      "\n",
      "BertConfig {\n",
      "  \"_name_or_path\": \"/tmp/facebook/esm2_t30_150M_UR50D\",\n",
      "  \"architectures\": [\n",
      "    \"EsmForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"emb_layer_norm_before\": false,\n",
      "  \"esmfold_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 640,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2560,\n",
      "  \"is_folding_model\": false,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"mask_token_id\": 32,\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 20,\n",
      "  \"num_hidden_layers\": 30,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"rotary\",\n",
      "  \"token_dropout\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_list\": null,\n",
      "  \"vocab_size\": 33\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, BertConfig\n",
    "from tape import ProteinBertConfig\n",
    "\n",
    "config1 = ProteinBertConfig.from_pretrained(\"bert-base\", num_labels=2)\n",
    "config2 = BertConfig.from_pretrained(\"../models/esm2_t30_150M_UR50D\", num_labels=2)\n",
    "\n",
    "print(config1)\n",
    "print(config2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "from tape import ProteinBertAbstractModel, ProteinBertModel\n",
    "\n",
    "from transformers import BertModel, BertPreTrainedModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers.file_utils import add_start_docstrings, add_start_docstrings_to_model_forward\n",
    "from transformers import Trainer, TrainingArguments, BertConfig\n",
    "from transformers.models.bert.modeling_bert import BERT_INPUTS_DOCSTRING, BERT_START_DOCSTRING\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, matthews_corrcoef, roc_auc_score\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"\"\"\n",
    "    Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled\n",
    "    output) e.g. for GLUE tasks.\n",
    "    \"\"\",\n",
    "    BERT_START_DOCSTRING,\n",
    ")\n",
    "class TapeLinear(ProteinBertAbstractModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = ProteinBertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids=None,\n",
    "            attention_mask=None,\n",
    "            token_type_ids=None,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            labels=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
    "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([ 0, 16,  9, 18, 18, 12,  5,  8,  6,  5,  5,  7, 13,  5, 12, 20,  9,  8,\n",
      "         8, 18, 13, 19, 18, 13, 12, 13,  9,  5, 11, 19, 21,  7,  7, 18, 11, 11,\n",
      "        12, 14,  4,  7,  5,  4, 11,  4, 11,  8, 19,  4,  6,  4, 15, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,  2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor(1)}\n",
      "torch.Size([71])\n",
      "{'input_ids': array([ 2, 20,  9, 10, 10, 13,  5, 22, 11,  5,  5, 25,  8,  5, 13, 16,  9,\n",
      "       22, 22, 10,  8, 28, 10,  8, 13,  8,  9,  5, 23, 28, 12, 25, 25, 10,\n",
      "       23, 23, 13, 19, 15, 25,  5, 15, 23, 15, 23, 22, 28, 15, 11, 15, 14,\n",
      "        3]), 'input_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1]), 'targets': array([0.698876, 1.      ])}\n",
      "(52,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(train_dataset[0][\\'input_ids\\'].shape)\\nprint(val_dataset[0])\\nprint(test_dataset[0])\\n\\nconfig = ProteinBertConfig.from_pretrained(\"bert-base\", num_labels=2)\\n#config = BertConfig.from_pretrained(\"bert-base\", num_labels=2)\\nconfig.rnn = \"lstm\"\\nconfig.num_rnn_layer = 2\\nconfig.rnn_dropout = 0.1\\nconfig.rnn_hidden = 768\\nconfig.length = 51\\nconfig.cnn_filters = 512\\nconfig.cnn_dropout = 0.1'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import MSELoss\n",
    "from transformers import BertModel, BertPreTrainedModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers.file_utils import add_start_docstrings, add_start_docstrings_to_model_forward\n",
    "from transformers.models.bert.modeling_bert import BERT_INPUTS_DOCSTRING, BERT_START_DOCSTRING\n",
    "from transformers import Trainer, TrainingArguments, BertConfig\n",
    "\n",
    "from tape.tokenizers import TAPETokenizer\n",
    "from tape import ProteinBertConfig\n",
    "\n",
    "from data_loader import My_Load_Dataset\n",
    "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, matthews_corrcoef, roc_auc_score\n",
    "\n",
    "from dataloader import BertDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import math\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    prediction=pred.predictions\n",
    "    preds = prediction.argmax(-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    precision = tp / (tp + fp) \n",
    "    recall = tp / (tp + fn)\n",
    "    sn = tp / (tp + fp)       \n",
    "    sp = tn / (tn + fp)  # true negative rate\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    auc = roc_auc_score(labels, preds)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return {\n",
    "        'auc': auc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'sn': sn,\n",
    "        'sp': sp,\n",
    "        'accuracy': acc,\n",
    "        'mcc': mcc\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "trainset = BertDataset('../dataset/netMHCIIpan3.2/train_micro.csv', max_pep_len=24)\n",
    "valset = BertDataset('../dataset/netMHCIIpan3.2/eval_micro.csv', max_pep_len=24)\n",
    "\n",
    "train_data = DataLoader(        trainset,\n",
    "                                batch_size=32,\n",
    "                                shuffle=True,\n",
    "                                num_workers=16,\n",
    "                                pin_memory=True,\n",
    "                                collate_fn=trainset.collate_fn)\n",
    "\n",
    "val_data = DataLoader(        valset,\n",
    "                              batch_size=64,\n",
    "                              num_workers=16,\n",
    "                              pin_memory=True,\n",
    "                              collate_fn=valset.collate_fn)\n",
    "\n",
    "model_name = \"../models/esm2_t6_8M_UR50D\"\n",
    "train_dataset = My_Load_Dataset(path=\"../dataset/netMHCIIpan3.2/train_micro.csv\", tokenizer_name=model_name, max_length=71)\n",
    "val_dataset = My_Load_Dataset(path=\"../dataset/netMHCIIpan3.2/eval_micro.csv\", tokenizer_name=model_name, max_length=71)\n",
    "\n",
    "print(train_dataset[0])\n",
    "print(train_dataset[0]['input_ids'].shape)\n",
    "print(trainset[0])\n",
    "print(trainset[0]['input_ids'].shape)\n",
    "\n",
    "\"\"\"\n",
    "print(train_dataset[0]['input_ids'].shape)\n",
    "print(val_dataset[0])\n",
    "print(test_dataset[0])\n",
    "\n",
    "config = ProteinBertConfig.from_pretrained(\"bert-base\", num_labels=2)\n",
    "#config = BertConfig.from_pretrained(\"bert-base\", num_labels=2)\n",
    "config.rnn = \"lstm\"\n",
    "config.num_rnn_layer = 2\n",
    "config.rnn_dropout = 0.1\n",
    "config.rnn_hidden = 768\n",
    "config.length = 51\n",
    "config.cnn_filters = 512\n",
    "config.cnn_dropout = 0.1\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
